{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import *\n",
    "import sys, time\n",
    "\n",
    "from pybrain.rl.environments.mazes import Maze, MDPMazeTask\n",
    "from pybrain.rl.learners.valuebased import ActionValueTable\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "from pybrain.rl.learners import Q, SARSA\n",
    "from pybrain.rl.experiments import Experiment\n",
    "from pybrain.rl.environments import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Para visualizar, inicializamos a plotting engine\n",
    "import pylab\n",
    "pylab.gray()\n",
    "pylab.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enviroment é o mundo onde o agente atua\n",
    "# input: performAction()\n",
    "# output: getSensors()\n",
    "# ambiente do exemplo: maze - cria um labirinto com campos livres, paredes e um ponto objetivo\n",
    "# o agente pode se movimentar nos campos livres, e precisar achar o ponto objetivo\n",
    "structure = array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "                   [1, 0, 0, 1, 0, 1, 1, 0, 1],\n",
    "                   [1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "                   [1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "                   [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "                   [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "# cria a estrutura do enviroment\n",
    "environment = Maze(structure, (7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# próximo passo, precisamos do agente, agente é onde o aprendizado ocorre\n",
    "# interage com o enviroment com getAction() e integrateObservation()\n",
    "# o agente consiste em:\n",
    "# um controller que mapeia estados a ações\n",
    "# um aprendiz que atualiza os parametros do controller de acordo com a interação com o enviroment\n",
    "# um explorador que adiciona alguns comportamentos explorativos as ações\n",
    "\n",
    "# o controller do pybrain é um módulo que pega estados como input e os tranforma em ações\n",
    "# para usar o q-learning precisamos de um módulo que implemente o ActionValueInterface\n",
    "# pybrain usa dois módulos pra isso: \n",
    "# ActionValueTable (para ações discretas) e ActionValueNetwork (para ações contínuas)\n",
    "# o enviroment de exemplo (maze), usa ações discretas - ActionValueTable\n",
    "\n",
    "controller = ActionValueTable(81, 4)\n",
    "controller.initialize(1.)\n",
    "\n",
    "# a tabela precisa do número de estados e ações como parâmetros, o exemplo tem 4 ações:\n",
    "# norte, leste, oeste e sul\n",
    "\n",
    "# depois inicializamos a tabela com 1 (não é necessário, mas ajuda a convergir mais rápido)\n",
    "# por que pares estado-ação não visitados tem um valor positivo e serão preferidos\n",
    "# ao invés de pares visitados que não levam para o objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cada agente tem um componente aprendiz, nesse caso utilizamos o q-learning, que é baseado em valores.\n",
    "learner = Q()\n",
    "agent = LearningAgent(controller, learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# até agora criamos o agente, o próximo passo é ligar o agente ao ambiente (enviroment)\n",
    "# para conectar o ambiente e o agente:  task\n",
    "# a task define qual é o objetivo e como o agente é recompensado por suas ações\n",
    "# para experimentos em episódios, task também define quando o episódio termina\n",
    "# ambientes geralmente tem suas próprias tasks\n",
    "# o ambiente de exemplo (maze) - tem a MDPMazeTask\n",
    "# MPD - Markov Decision Process: define que o agente sabe sua localização no ambiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-6740a6fc7cc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# então, criamos um experimento, e deixamos ambos agente e task rodar sobre alguns passos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexperiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoInteractions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'task' is not defined"
     ]
    }
   ],
   "source": [
    "# então, criamos um experimento, e deixamos ambos agente e task rodar sobre alguns passos\n",
    "experiment = Experiment(task, agent)\n",
    "\n",
    "while True:\n",
    "    experiment.doInteractions(100)\n",
    "    agent.learn()\n",
    "    agent.reset()\n",
    "\n",
    "    pylab.pcolor(controller.params.reshape(81,4).max(1).reshape(9,9))\n",
    "    pylab.draw()\n",
    "    \n",
    "# esse experimento executa 100 interações entre agente e ambiente (agente e task)\n",
    "# task processa a ação do agente, escala e entrega para o ambiente (enviroment)\n",
    "# ambiente responde, retorna o novo estado para a task que decide que tipo de informação é dada para o agente.\n",
    "# a task também dá uma recompensa para cada passo do agente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
